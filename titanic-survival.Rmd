---
title: |
  Modeling Titanic Survival
author:
  - name: Qiushi Yan
    affil: ""
    email: "qiushi.yann@gmail.com, website: https://qiushi.rbind.io"
affiliation:
  - num: a
    address: |
      Beijing, China
abstract: |
   This case study showcases the development of a binary logistic model to predict the possibility of survival in the loss of Titanic. I demonstrate the overall modeling process, including preprocessing, exploratory analysis,  model fitting, adjustment, bootstrap validation and interpretation as well as other relevant techniques such as redundancy analysis and multiple imputation for missing data. The motivation and justification behind critical statistical decisions are explained. This analysis is fully reproducible with all source R code and text. 
bibliography: references.bib
link-citations: true
geometry: "margin=1in"
colorlinks: yes
#appendix: appendix.tex
output:
  bookdown::pdf_book:
    base_format: rticles::tf_article
    includes:
      in_header: header.tex
    dev: "cairo_pdf"
---

http://www.crema-research.ch/papers/2009-03.pdf

Who Survived Titanic? A Logistic Regression Analysis: https://sci-hub.do/https://journals.sagepub.com/doi/pdf/10.1177/084387140401600205


https://www.insider.com/titanic-secrets-facts-2018-4#at-the-memorial-of-frederick-fleet-one-of-the-lookouts-a-prankster-left-a-pair-of-binoculars-with-a-note-reading-sorry-for-bringing-these-100-years-too-late-8


http://rpubs.com/edwardcooper/titanic1

https://www.kaggle.com/mrisdal/exploring-survival-on-the-titanic/report

https://www.kaggle.com/startupsci/titanic-data-science-solutions/comments

https://www.newscientist.com/article/dn22119-sinking-the-titanic-women-and-children-first-myth/

```{r, include = FALSE}
# global chunk options 
knitr::opts_chunk$set(
  warning = FALSE,
  message = FALSE,
  comment = "",
  fig.align = "center",
  echo = FALSE
)

# load libraries 
library(dplyr) # data wrangling
library(ggplot2) # visualization
library(rms) # modeling
library(mice) # imputation for missing data
library(patchwork) # plot composition
library(rpart) # decision tree for mis
# latex print option
options(prType='latex')
# load raw data
t <- readr::read_csv("data/titanic.csv") 
# modify Hmisc::latex
mylatex <- function (...) {
    o <- capture.output(latex(...))
    # this will strip /all/ line-only comments
    o <- grep('^%', o, inv=T, value=T)
    cat(o, sep='\n')
}
# theme for ggplot2
ggplot2::theme_set(ymisc::theme_clean())
```




# Introduction 

The sinking of RMS Titanic brought to numerous machine learning competitions a quintessential dataset among others. After the "unsinkable" British passenger liner struck an iceberg in her maiden voyage on 15 April 1912 and was eventually wrecked, more than 1500 people perished. Decades of effort has been devoted to the study of the historic event, in which one major interest for statistical inquiries is to model and predict survival given a number of characteristics, since there was clear account that some people  were allowed to get on the lifeboat first.  

There are numerous variants of Titanic data existed on the web, with primary source based on [Encyclopedia Titanica](https://www.encyclopedia-titanica.org/) [-@hind],  a site started in 1996 as an attempt to tell the story of every person that traveled the Titanic as a passenger or
crew member. This project is based on the most recent version as of October 2020, with following columns available (table \@ref(data-dictionary)). Source data and steps of data cleaning are elaborated in the [data](#data) section in the appendix. 



```{r, results="asis"}
tribble(
  ~ Variable, ~ Definition, ~ Note,
  "survived", "Survival Status", "0 = Lost, 1 = Saved",
  "age", "Age", "In years, some infants had fractional values",
  "gender", "Gender", "",
  "class", "Cabin class", "1st, 2nd, 3rd or Crew",
  "nationality", "Motherland", "from wiki passenger list",
  "title", "Title", "Extracted from name",
  "spouse", "\\# of spouse on board", "",
  "sibling", "Number of siblings on board", "",
  "parent", "Number of parents on board", "",
  "children", "Number of children on board", "",

) %>% 
  mylatex(file = "", 
          table.env = TRUE, label = "data-dictionary", size = "small",
          caption = "Cleaned data with 2208 rows and 11 columns", rowname = NULL, where = "h")
```


After appropriate formatting and cleaning, the data at hand recorded the survival status `r nrow(t)` Titanic travelers alongside his/her gender, age, companions on board, title, nationality, etc. There were `r (t %>% count(survived))[["n"]][[1]]` victims and `r (t %>% count(survived))[["n"]][[2]]` survivors in total. 

It is essential for every fruitful task of data analysis to first identify key questions of investigation that facilitates interpretation, however vague they are at the beginning. Then we can approach the core problem, filtering out trivialities, with statistical expression by abstraction. For our purposes, we could establish the following questions for which to quest

- To which degree is *Women and children first* policy respected? After the collision, the captain explicitly issued an order for women and children to be saved first.^[Though there is no international maritime law enforcing this kind of chivalry.] Thus we should expect significantly higher proportion of females and children rescued than that in males and adults. If the opposite is true, that Titanic subjects behave more in line with the selfish *homo oeconomicus*, where everybody looked out for himself or herself and possibly even puts other peopleâ€™s lives in danger, then people in their prime with physical superiority would see higher probability of survival. This requires us to study gender and age effect. 


- Did socio-economic advantages mean better chance of survival? If this is the case, passengers with higher financial means, i.e. who live in the first class are more likely to survive. Similarly,  passengers from second class will have a higher change of survival than third class people. Cabin class's impact on survival status needs special notice here. 

- For those who traveled alone with no companions (spouse, sibling, parent, children) on the vessel, is their survival possibility greater or less? On one hand, they are more likely to be in shortage of psychological and physical support. On the other hand, they would may be able to reach a life-saving decision faster without transaction cost and negotiation.  

- Did English subjects receive any special care or given priority to aboard lifeboats? After all, Titanic was operated by British crew, and managed by British captain, masters and officers. Conversely, British nobility and elite 

- Quantify interactions among various characteristics. Specifically, there are important interactions that need extra notice. For example, it has been widely studied in sociology and anthropology that human are sometimes driven by *procreation instinct* so that social norms would entail needs to protect females of reproductive age [@frey2009surviving].^[The average peak reproductive period in females is between the ages of 16 and 35.] Therefore, we could specify and study the interaction between age and gender. Another typical interaction is between offspring and gender. *Parental investment* suggest that women on average invest more in caring for their offspring than males. In times of a disaster,  higher opportunity cost will alert females with offspring more than others, and make them seek more aggressively for changes to secure the children as well as themselves.  


This case study has been greatly inspired by Dr. Frank Harrell's similar example in his *Regression Modeling Strategies* [-@harrell2015regression Chapter 12] book, here I attempt to propose my understanding and interpretation of model development that is as original as possible. To ensure reproducibility, all the analysis is done in R [@base] with code and text made public in this [repo](https://github.com/enixam/titanic-survival). A brief summary of each section is listed below 

- [Exploration](#exploration). Use descriptive statistics to examine data distribution characteristics, data missing patterns and relative effects, followed by redundancy analysis to study dependencies among predictors. Finish with nonparametric loess regression exploring nonlinear trends. 

- [Model development](#dev). The key section in specifying, developing, validating and describing a binary logistic model, split into 

  - [Specification](#spec) Prespecification of predictor complexity with a saturated main effect model. 
  
  - [Multiple imputation](#imputation): Use predictive mean matching to impute subject's age, resulting in 30 complete datasets. 
  
  - [Model fitting, validation and calibration](#fit). Obtain pooled parameter estimates based on prespecified complexity and imputation results. Use bootstrap validation and calibration curve (the ".632" method) to study model performance and optimism. 

  - [Interpratation](#interpretation). Summarize the model with estimation and hypothesis testing, combined with graphical methods like partial effect plots and nomogram. 
  
  
- [Discussion](#discussion). Model-based explanation to address some of our former questions. 

- [Conclusion](#conclusion). Conclusion and further study.  



# Exploration {#exploratory}

## Descriptive statistics and data processing

A graphical summary of of the data is given by the `Hmisc::describle` function. For numerical variables, a inline histogram is produced alongside summary measures such as the number of missing values and the mean. For discrete variables, we focus on the number of categories and their relative frequency.  

```{r, echo = TRUE, results = "asis"}
# print a summary for the data
t %>% 
  describe() %>%
  latex(file = "", size = "small", center = "none")
```


There are several noteworthy patterns.^[Though this may not be relevant to the model, it is still an surprising discovery that it wasn't until the late 19th century that the idea of women traveling alone gained ground. As a result, there were nearly twice as many males passengers as females on Titanic. In fact, only 40% female passengers have no companion on the ship.] 

Of special importance is the `age` variable, which has roughly 30% missingness. On the other hand, it has a nearly symmetric distribution with 80% known observations falling between 14 and 50. For further examination of patterns of missing data, we could fit a decision tree to predict which type of subject tend to have missing ages. Generally, for some third class male passenger or crew, age is mostly to miss. 

```{r na-tree, echo = TRUE, fig.cap = "(ref:na-tree)", fig.height = 3.5}
na_tree <- rpart(factor(is.na(age)) ~ ., 
                 data = t %>% mutate(survived = as.factor(survived)) , 
                 minbucket = 50)
# figure 1
rpart.plot::rpart.plot(na_tree, type = 3, cex = 0.6)
```

(ref:na-tree) The decision tree for predicting `is.na(age)`, which finds strong patterns of missing related to class/department and  gender (the Syrian node has very limited samples). Each node shows (top to bottom) the predicted class, the predicted probability of age being missing, the percentage of observations in the node.   


We see in figure \@ref(fig:na-tree) that survival status, gender and class are essential in determining age missingness. For a 3rd class male passenger who did not survive, age is missing with a probability of 60%. Interestingly, English male crew members are much more likely to have missing age than subjects of other nationality


Back to other variables in descriptive statistics. Distributions of subject's companion on Titanic are all too narrow, as shown in figure \@ref(fig:skew-rel). This motivates categorization since we will not lose too much information. Lastly, nearly half of the subjects are English. And if we focus on crew, the number rise to 85%. 


```{r skew-rel, fig.cap = "(ref:skew-rel)", fig.height = 3.5, fig.width = 5}
t %>%
  tidyr::pivot_longer(8:11, names_to = "relation", values_to = "n") %>%
  ggplot() + 
  geom_bar(aes(n), color = "black", fill = "midnightblue", alpha = 0.7) + 
  facet_wrap(~ relation, nrow = 2) + 
  labs(x = NULL, y = NULL) + 
  scale_y_log10()
```

(ref:skew-rel) Few subjects have more than one companion in any of the 4 relations. Y axis on log scale.

Given this results, the final step in data munging is to dichotomize `spouse`, `parent`, `children` and `sibling` to denote if there is such relation. Thus we no longer have to deal with continuous predictors with poor distribution. 

```{r}
t$spouse <- if_else(t$spouse == 0, "0", "1")
t$parent <- if_else(t$parent == 0, "0", "1")
t$sibling <- if_else(t$sibling == 0, "0", "1")
t$children <- if_else(t$children == 0, "0", "1")
```

Univariate relationship between each independent variable and survival status is presented in figure \@ref(fig:univariate). For each column, we can build a anova-type plot with no control over confounding variables, though it may still assist us in determining how to spend degrees of freedom. If a predictor's effect on the response is strong, it's more likely that we need to spend more parameters on it. However, if a variable's effect appears to be weak, it could either due to a truly flat relationship, or to nonlinearity and predictors among variables that univariate method cannot detect. 

```{r univariate, fig.height = 7.5, fig.width  = 3.5, fig.cap = "Summary of relationship between survival and each predictor"}
s <- summary(survived ~ ., 
    data = t)
plot(s, main = "" , subtitles = TRUE, cex = 0.65, pch = 22, width = 3)
```

The plot shows appreciably strong effects of gender and cabin class on survival status. The effect of age seems trivial except for the missing subjects, but again, this figure exposes only linear relationship, and only after categorization. As we will see in the next section, age effect are much nonlinear and concentrated in the young subjects. The downside of this kind of univariate relationship is also exemplified in `title`, where "Miss". For the same reason effects of other variables cannot be determined. 

We will finish with a redundancy analysis to study if any predictor can be readily explained by the rest of predictors, therefore does not much bring new information and may not enter the model.  The checking algorithm involves 

```{r, comment = ""}
redun(~ age  + class +  nationality + title 
      , data = t)
```





## Loess regression for nonlinear pattern 

The loess method is a common nonparametric regression model to study nonlinear relationship. In the case of binary response, the fitted value at $x = x_0$ is the weighted proportion of positive cases near the neighborhood of $x_0$. If the trend of a loess curve shows nonmonotoncity, it is reasonable to include that nonlinearity relationship in the model, e.g., modeling the predictor with polynomial transformation or with splines. 

Another important interaction, according to many follow up studies, is related to cabin class (for passenger) and department (for crew and staff). 

\@ref(fig:loess-curve)

```{r loess-curve, fig.height = 7.5, fig.cap = "(ref:loess-curve)"}
p1 <- ggplot(t, aes(age, survived)) +
  histSpikeg(survived ~ age, lowess = TRUE, data = t) + 
  labs(y = NULL, x = NULL)
p2 <- ggplot(t, aes(age, survived, color = gender)) + 
  histSpikeg(survived ~ age + gender, lowess = TRUE, data = t) + 
  labs(y = NULL, x = NULL)
p3 <- ggplot(t, aes(age, survived, color = class)) + 
  histSpikeg(survived ~ age +  class + gender, lowess = TRUE, data = t) + 
  labs(y = NULL) + 
  facet_wrap(~ gender)
(p1 + p2) / p3 
```

(ref:loess-curve) `loess` estimates of $P(\text{survived})$, with tick marks representing frequency counts within equal-width bins. Top left panel shows the nonlinear relationship between age and survival status without controlling confounding variables. Other plots give estimates under stratification by sex and class.  







# Model development {#dev}

A typical modeling workflow begins with an choice of a statistical model or a machine learning model. A statistical model often stems from a hypothesized probabilistic data generating mechanism and assumes additivity, whereas machine learning models is algorithmatic, optimized through parameter tuning to achieve a higher performance score. We choose the "simple" binary logistic model for the following reasons. 

We prefer probabilistic predictions to classification with output label 0 and 1, since we are placing emphasis upon the *tendency* of survival. And the value of our model consist not in a dichotomous prediction, but in what characteristics would increase or decrease the possibility of survival. The notion has ruled out most of the machine learning models for classification, say, random forest, support vector machines and neural network, which are not intrinsically probability oriented. Such classifiers can often only yield a forced choice. 

Interpretability and inference matters. Although some top data science competitioners has reported moderately high signal to noise ratio (e.x., 90% prediction accuracy) that might tip the balance towards  machine learning models, interpretability is harmed. Specifically, statistical models favour additivity have explicit specification. As a result, there are natural distinctions between main effects and interactions, linearity and nonlinearity. And the inference procedure is well defined provided that the model is correctly specified. While in a multilayer neural network, everything can interact with one another and it could be daunting to isolate effects and conduct former inference. 

Machine learning models are data hungry and sometimes create the need for big data [@van2014modern]. To guard against overfitting, the analyst has to have a sample size that is 10 times larger at least if he chooses a decision tree instead of regression models. The rationale is that a statistical model is a safer approach as Dr. Harrell commented

> If n is too small to do something simple, it is too small to do something complex


## Specification {#spec}

We start by fitting a relatively large model, to decide how model complexity should be properly represented. This includes deciding the number of knots for continuous predictors and the number of categories of categorical predictors, could we remove some term, where should we place interaction, etc. The large model also gives an overall sense of the predictive ability of each subject characteristics on survival status. 

This is done by developing a saturated logistic model, with maximum flexible nonlinear age effect represented as natural splines with 5 knots, and with all categorical predictors retain their original categories without pooling. Two way interactions have been specified between age and gender, age and class, and age and parent. We will not create three-way or higher interactions to avoid singularity. Since this is an initial model, observations with missing age are not used. The model equation is 

```r
survived ~ (rcs(age, 5) + gender + class)^2 + (rcs(age, 5) * parent) + 
            joined + spouse + sibling + 
            children + nationality + title
```

```{r}
dd <- datadist(t)
options(datadist = "dd")
f1 <- lrm(survived ~ (rcs(age, 5) + gender + class)^2 +
            (rcs(age, 5) * parent) + joined + 
            spouse + sibling + children + 
            nationality + title,
          data = t, x = TRUE, y = TRUE)
```



```{r, results="asis"}
mylatex(anova(f1), file="", size = "small", table.env = TRUE, caption = "Hypothesis testing for the saturated model", label = "fone-anova")
```

Table \@ref(fone-anova) 

$\chi^2 - \text{df}$ is the "adjusted" 



anova plot

```{r}
plot(anova(f1))
```

Great care should be taken when one attempts to conduct model simplification based on hypothesis testing and p-values in table \@ref(saturated-table). Deletion of a predictor whose p-value is, say, 0.08, could leads to severe problems phantom degrees of freedom that distort coefficient estimates, confidence intervals, p-value and calibration of the final model [@grambsch1991effects]. A more reliable way is to use some form of variable selection over large number of resamples, and count the number of times a predictor is selected in the final model. 


```{r, include = FALSE}
set.seed(2021)
v1 <- validate(f1, B = 200, bw = TRUE)
```

```{r, results = "asis"}
attr(v1, "kept") %>% 
  as_tibble() %>% 
  summarise(across(everything(), sum)) %>% 
  select(where(~ . <= 20)) %>% 
  mylatex(file = "", 
          table.env = TRUE, label = "boot-bw", size = "small",
          caption = "Terms retained less than 15 times in the model in 200 bootstrap resamples", rowname = NULL, where = "h")
```

Table \@ref(boot-bw) lists predictors with less than 15 presences in the final model during backward selection in 200 bootstrap resamples. These resampling results combined with p-values are then used to guide the pruning of the saturated model. 

```{r, results = "asis"}
tribble(
  ~ "age", ~ "gender", ~ "class",
  4, 1, 1,
  4, 4, 4
) %>% mylatex(size = "small", file = "", table.env = TRUE, caption = "(ref:df-budget)", rowname = NULL, where = "h")
```

(ref:df-budget) d.f. budget in the saturated model. Row 1: main effects. Row 2: interactions. 





## Multiple imputation {#imputation}

The goal of multiple imputation is to provide an accurate estimate of the variance-covariance matrix that does not only accounts for sampling variability, but also for the extra variance caused by missing values and finite number of imputations [@van2018flexible]. As a result, tests on individual parameters gain power and bias are reduced. The general idea is to generate multiple complete dataset, fit the model in parallel, and then obtain a pooled final estimate by averaging over all fitted models. 


In this case study, we need only to impute `age`. The sole decision to be made 


```{r, cache = TRUE}
# use predictive mean matching to generate 30 complete dataset
imp <- mice(t, method = "pmm", m = 30, 
            printFlag = FALSE)
```

```{r, fig.width = 4.5, fig.height = 3, fig.cap = "Density plot of observed and imputed data. In general, the imputed dataset mimic the age distribution seen in the observed data."}
densityplot(imp)
```














## Model fitting, validation and calibration {#fit}


```{r, include = FALSE}
f2 <- fit.mult.impute(survived ~ (rcs(age, 5) + gender + class)^2 +
            rcs(age, 5) * parent +  nationality + joined,
            fitter = lrm, 
            imp, 
            data = t)
```

anova plot


```{r, results = "asis"}
mylatex(anova(f2), file = "", size = "small", table.env = TRUE, label = "ftwo-anova",
        caption = "Hypothesis testing for the final model")
```

\@ref(ftwo-anova)

Indexes of model performance. One may absolute accuracy (proportion classified correctly), sensitivity, specificity, precision, and recall are all improper accuracy

\begin{table}[h]
{\small
\caption{Model index\label{model-index}} 
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
&Model Likelihood&Discrimination&Rank Discrim.\\
&Ratio Test&Indexes&Indexes\\\hline
Obs~\hfill 2208&LR $\chi^{2}$~\hfill 783.83&$R^{2}$~\hfill 0.418&$C$~\hfill 0.813\\
~~0~\hfill 1496&d.f.~\hfill 41&$g$~\hfill 1.799&$D_{xy}$~\hfill 0.626\\
~~1~\hfill 712&Pr$(>\chi^{2})$~\hfill \textless 0.0001&$g_{r}$~\hfill 6.053&$\gamma$~\hfill 0.628\\
$\max|\frac{\partial\log L}{\partial \beta}|$~\hfill 0.007&&$g_{p}$~\hfill 0.274&$\tau_{a}$~\hfill 0.274\\
&&Brier~\hfill 0.144&\\
\hline
\end{tabular}
\end{center}}
\end{table}



Point estimate, standard error, Wald statistic and individual p-values.

\setlongtables\begin{longtable}{lrrrr}
\hline
\multicolumn{1}{l}{}&\multicolumn{1}{c}{$\hat{\beta}$}&\multicolumn{1}{c}{S.E.}&\multicolumn{1}{c}{Wald $Z$}&\multicolumn{1}{c}{Pr$(>|Z|)$}\tabularnewline
 \hline
 \endhead
 \hline
 \endfoot
 Intercept&~ -0.1475~&~ 1.9879~&-0.07&0.9409\tabularnewline
 age&~  0.2225~&~ 0.1244~& 1.79&0.0737\tabularnewline
 age'&~ -0.9850~&~ 0.6877~&-1.43&0.1520\tabularnewline
 age''&~  4.4844~&~ 3.6008~& 1.25&0.2130\tabularnewline
 age'''&~ -5.5980~&~ 4.9656~&-1.13&0.2596\tabularnewline
 gender=Male&~ -1.8842~&~ 1.0197~&-1.85&0.0646\tabularnewline
 class=2nd&~  5.4575~&~ 2.8123~& 1.94&0.0523\tabularnewline
 class=3rd&~ -1.0535~&~ 1.8583~&-0.57&0.5708\tabularnewline
 class=crew&~ -2.4149~&~ 3.2651~&-0.74&0.4595\tabularnewline
 parent=1&~  3.4640~&~ 1.2035~& 2.88&0.0040\tabularnewline
 nationality=English&~  0.0730~&~ 0.2757~& 0.26&0.7912\tabularnewline
 nationality=Finnish&~  0.1843~&~ 0.4289~& 0.43&0.6674\tabularnewline
 nationality=Irish&~  0.0116~&~ 0.3901~& 0.03&0.9763\tabularnewline
 nationality=Other&~ -0.2328~&~ 0.2669~&-0.87&0.3832\tabularnewline
 nationality=Swedish&~ -0.2673~&~ 0.3820~&-0.70&0.4840\tabularnewline
 nationality=Syrian&~ -0.0582~&~ 0.4429~&-0.13&0.8955\tabularnewline
 joined=Cherbourg&~  0.6565~&~ 0.3321~& 1.98&0.0480\tabularnewline
 joined=Queenstown&~  0.0538~&~ 0.4133~& 0.13&0.8965\tabularnewline
 joined=Southampton&~ -0.0632~&~ 0.2003~&-0.32&0.7523\tabularnewline
 age $\times$ gender=Male&~ -0.1498~&~ 0.0573~&-2.61&0.0090\tabularnewline
 age' $\times$ gender=Male&~  0.8213~&~ 0.3849~& 2.13&0.0329\tabularnewline
 age'' $\times$ gender=Male&~ -3.9560~&~ 2.3149~&-1.71&0.0875\tabularnewline
 age''' $\times$ gender=Male&~  4.8271~&~ 3.5665~& 1.35&0.1759\tabularnewline
 age $\times$ class=2nd&~ -0.3846~&~ 0.1754~&-2.19&0.0283\tabularnewline
 age' $\times$ class=2nd&~  1.4771~&~ 0.8999~& 1.64&0.1007\tabularnewline
 age'' $\times$ class=2nd&~ -6.7250~&~ 4.5517~&-1.48&0.1395\tabularnewline
 age''' $\times$ class=2nd&~  8.5082~&~ 6.1731~& 1.38&0.1681\tabularnewline
 age $\times$ class=3rd&~ -0.1403~&~ 0.1159~&-1.21&0.2262\tabularnewline
 age' $\times$ class=3rd&~  0.7699~&~ 0.6331~& 1.22&0.2240\tabularnewline
 age'' $\times$ class=3rd&~ -4.9212~&~ 3.2961~&-1.49&0.1354\tabularnewline
 age''' $\times$ class=3rd&~  7.7752~&~ 4.5292~& 1.72&0.0860\tabularnewline
 age $\times$ class=crew&~  0.0589~&~ 0.1890~& 0.31&0.7554\tabularnewline
 age' $\times$ class=crew&~ -0.0468~&~ 0.8023~&-0.06&0.9535\tabularnewline
 age'' $\times$ class=crew&~ -0.5748~&~ 3.6057~&-0.16&0.8733\tabularnewline
 age''' $\times$ class=crew&~  1.5477~&~ 4.4606~& 0.35&0.7286\tabularnewline
 gender=Male $\times$ class=2nd&~ -0.4060~&~ 0.7087~&-0.57&0.5667\tabularnewline
 gender=Male $\times$ class=3rd&~  2.1475~&~ 0.6288~& 3.41&0.0006\tabularnewline
 gender=Male $\times$ class=crew&~  0.5495~&~ 0.8726~& 0.63&0.5289\tabularnewline
 age $\times$ parent=1&~ -0.1190~&~ 0.1118~&-1.06&0.2871\tabularnewline
 age' $\times$ parent=1&~ -0.6885~&~ 1.1693~&-0.59&0.5560\tabularnewline
 \newpage
 age'' $\times$ parent=1&~ 10.4417~&~11.6224~& 0.90&0.3690\tabularnewline
 age''' $\times$ parent=1&~-29.7852~&~35.4739~&-0.84&0.4011\tabularnewline
 \hline
 \end{longtable}
 \addtocounter{table}{-1}






There will not be another Titanic, and any model on Titanic will not be used for prediction. Therefore, the goal of model validation is primarily to provide quantify the degree of overfitting with various bias-corrected measures. More accurately, we will be using bootstrap internal validation. 

The van Houwelingenâ€“Le Cessie heuristic shrinkage estimate

$$
\hat{\gamma} = \frac{\text{model } \chi^2 - p}{\text{model } \chi^2}
$$
where $p$ is the total degrees of freedom and $\chi^2$
the global likelihood ratio statistic for all predictors. 


In the award-winning solution to this legendary dataset presented by IBM Watson, they used a holdout sample to validate their model.  https://www.fharrell.com/post/split-val/


validation 

```{r, include = FALSE, cache = TRUE}
f2 <- update(f2, x = TRUE, y = TRUE)
v2 <- validate(f2, B = 200)
```



```{r, results = "asis"}
mylatex(v2, file = "")
```




The area under the ROC curve as well as the concordance probability is `r (v2["Dxy", "index.orig"] / 2) + 0.5`

$$
D_{xy} = 2(c - 0.5)
$$

The 45 degree line indicates the ideal scenario in which prediction perfectly matches observation. 


```{r, fig.cap = "Calibration curve of the model output probabilites on resampled data"}
cal <- calibrate(f2, B = 200)
plot(cal, subtitles = FALSE)
```



As a integral component of model validation, calibration aims to gauge the concordance between predicted values and observed data. 

## Interpretation 



influence

`which.influence`


```{r, fig.height = 10, fig.cap = "nomogram"}
plot(nomogram(f2), cex.var = 0.8, cex.axis = 0.6)
```






https://www.encyclopedia-titanica.org/community/threads/passengers-who-spoke-other-languages.20103/

Since the crew's instructions (in English) tended to be along the lines of "Wait down here for further orders" a lack of understanding might well have saved many lives. Also many of the immigrants in 3rd class were traveling in family or neighborhood groups which included at least one English-speaker (often an established immigrant returning to the US from a visit back home) who could act as their spokesperson.




# Discussion 


The most decisive explanation for such effect is that first-class passengers had better access to information about the
imminent danger and were aware that the lifeboats were located close to the first class cabins. Thus, their marginal effort costs to survive were lower. In contrast, most third-class passengers had no idea where the lifeboats were located (safety drills for all passengers were introduced after the Titanic disaster), and they did not know how to reach the upper decks where the lifeboats were stowed.  

Wyn Craig Wade: there was a class culture on Titanic akin to the notion of a "culture of poverty

> Undoubtedly, the worst barriers were the ones within the steerage passengers themselves. Years of conditioning as third-class citizens led a great many of them to give up hope as soon as the crisis became evident ... Barriers to steerage? Yes, but of a kind less indictable to the White Star Line than to the whole of civilization.


A more detailed explanation of some of these measures is presented in the [appendix](#measures). 

*Women and children first only for higher class passengers*. If you are a third class female



# Conclusion 





\pagebreak


# (APPENDIX) Appendix {-} 


# Data

A variety of other versions and forms of Titanic data sources have been collected due to public's constant interests in the tragedy as well as modern efforts trying to unveil the mystery. A comprehensive overview of several data variants is given by @symanzikunsinkable. Data in this case study is accessed on [Encyclopedia Titanica](https://www.encyclopedia-titanica.org/), a leading archive on titanic facts.  In contrast to the the famous titanic dataset (known as `titanic3`) distributed by [kaggle](kaggle.com) for introductory level machine learning practices, the case study uses a more up-to-date and complete dataset in the following ways

- **Larger sample size**. Our data includes crew and staff members alongside passengers, while titanic3 only incorporate passenger information. We do not use a separate test set approach for validation either. As a result, the sample size is about 2.5 times larger. 

- **More columns**. Additional variables such as role on the ship, nationality and occupation are added. A major difference is made by separating the travel companion data into four distinct columns: number of parents, children, sibling and spouses that each passenger traveled with. These were combined into two columns before. 

- **More accurate**. `titanic3` was an effort to study Titanic in the 20th century, lastly updated and improved by Thomas Cason in 1999. During the recent two decades the data has been constantly revised, many errors corrected, many missing ages filled in, and new variables created. Now it reflects our our most up-to-date understanding of the event, in the digital form, as of 21 October 2020. 


The data cleaning process involves using appropriate data types, creating new features, adjusting levels for categorical variable and excluding irrelevant columns. Code can be found at [clean.R](https://github.com/enixam/titanic-survival/clean.R).

`title` is extracted through each person's name with regular expressions and then collapsed into 4 levels.^[For example, the title for passenger "Abbing, Mr Anthony" is "Mr".]


Passengers are classified according to their cabin class. Others on the vessel fall into one of crew and staff members. Crew includes victualling crew^[crew in charge of food, housekeeping, laundry, room service, etc.], engineering crew, deck crew and officers, substitute crew and guarantee group. Staff members include restaurant staff and orchestra. 

Rare nationality (lower than 50 people) is collapsed. 

Age information is presented as non-missing on the surface yet there is an indicator column representing when a person's age is only approximate and cannot be fully determined from current facts. These inaccurate age have been assigned NA. There were also ten subjects whose four companion variables were all explicitly missing. For simplicity, the mode `0` is filled in. Therefore, the problem of missing data is reduced to univariate missing of `age`.   

Variables we do not utilize in this project includes name, date of birth and death, lifeboat number^[There were 9 recorded passengers who got on the lifeboat yet died before reaching Carpathia, another RMS which spearheaded the rescue of Titanic survivors. There were also 13 passengers who survived with no boat information documented, and this is most likely due to data quality issues after looking up on Encyclopedia Titanica. Even with these exceptions, whether a passenger got on a lifeboat yields perfect prediction on his/her survival. If one fits a logistic regression model on survival based on whether `boat` is missing, the apparent accuracy will be nearly 1. In this sense `boat` is more the result of survival, rather than a cause.], fare, and cabin number.^[While some study used this attribute to find cabin locations, its large amount of missingness could be a major source of complexity.]

# Model formula 

The formula for our binary logistic model 





# Criterion used in model validation {#measures}

Somer's $D_{xy}$ index is a calibration measure, which is the rank correlation between predicted and actual response. It has a close relationship with the C index 



# Computing environment

```{r, comment = "", echo = TRUE}
sessionInfo()
```


```{r bib,include=FALSE,cache=FALSE, eval = FALSE}
# automatically create a bib database for R packages
bib <- knitr::write_bib(
  x = c(
    .packages(), "knitr", "rmarkdown", "bookdown", "rticles"
  ), file = NULL, prefix = ""
)
bib <- unlist(bib)
# remove the ugly single quotes required by CRAN policy
bib <- gsub("(\\\n)", " ", bib)
readr::write_lines(bib, "references.bib", append = TRUE)
```

\nocite{*}
